{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models.segmentation import fcn_resnet50\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "from PIL import Image\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 0: BACKGROUND\n",
    "- 1: SKY\n",
    "- 2: VEGETATION\n",
    "- 3: BUILDING\n",
    "- 4: WINDOW\n",
    "- 5: GROUND\n",
    "- 6: NOISE \n",
    "- 7: DOOR "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASS_COLORS = {\n",
    "    0: (0, 0, 0),\n",
    "    1: (0, 0, 255),\n",
    "    2: (0, 255, 0),\n",
    "    3: (0, 125, 125),\n",
    "    4: (255, 255, 0),\n",
    "    5: (125, 125, 125),\n",
    "    6: (255, 0, 0),\n",
    "    7: (125, 125, 0)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, img_dir, mask_dir, class_colors, transform = None, mask_transform = None):\n",
    "        self.__img_dir = img_dir\n",
    "        self.__mask_dir = mask_dir\n",
    "        self.__image_filenames = os.listdir(mask_dir)\n",
    "        self.__class_colors = class_colors\n",
    "        self.__img_transform = transform\n",
    "        self.__mask_transform = mask_transform\n",
    "\n",
    "    # Convert a rgb mask in a index mask\n",
    "    def __rgb_mask_to_class(self, mask):\n",
    "\n",
    "        mask_array = np.array(mask) #PILImage don't have the attribute shape\n",
    "        label_mask = np.zeros((mask_array.shape[0], mask_array.shape[1]), dtype=np.uint8)\n",
    "        for idx, value in self.__class_colors.items():\n",
    "            label_mask[np.all(mask_array == value, axis=-1)] = idx\n",
    "        return Image.fromarray(label_mask)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.__image_filenames)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.__img_dir + self.__image_filenames[idx]\n",
    "        mask_path = self.__mask_dir + self.__image_filenames[idx]\n",
    "\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        mask = Image.open(mask_path).convert(\"RGB\")\n",
    "\n",
    "        mask = self.__rgb_mask_to_class(mask)\n",
    "\n",
    "        if self.__img_transform:\n",
    "            image = self.__img_transform(image)\n",
    "\n",
    "        if self.__mask_transform:\n",
    "            mask = self.__mask_transform(mask)\n",
    "\n",
    "        mask = torch.tensor(np.array(mask), dtype=torch.long)\n",
    "\n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(), # Transform to Tensor and set the shap [C, H, W]. It works only with RGB or PIL Images\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) # used with pretrained models\n",
    "])\n",
    "\n",
    "mask_transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256), interpolation=Image.NEAREST),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset('./TMBuD/images/', './TMBuD/gt_label/', CLASS_COLORS, image_transform, mask_transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\RaimondoReggio\\anaconda3\\envs\\hammon\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\RaimondoReggio\\anaconda3\\envs\\hammon\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FCN_ResNet50_Weights.COCO_WITH_VOC_LABELS_V1`. You can also use `weights=FCN_ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model = torchvision.models.segmentation.fcn_resnet50(pretrained=True)\n",
    "num_classes = len(CLASS_COLORS)\n",
    "model.classifier[4] = nn.Conv2d(512, num_classes, kernel_size=1)\n",
    "model.aux_classifier[4] = nn.Conv2d(256, num_classes, kernel_size=1)\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Loss: 0.7665\n",
      "Epoch [2/50], Loss: 0.4607\n",
      "Epoch [3/50], Loss: 0.3632\n",
      "Epoch [4/50], Loss: 0.3065\n",
      "Epoch [5/50], Loss: 0.2697\n",
      "Epoch [6/50], Loss: 0.2532\n",
      "Epoch [7/50], Loss: 0.2130\n",
      "Epoch [8/50], Loss: 0.1929\n",
      "Epoch [9/50], Loss: 0.1869\n",
      "Epoch [10/50], Loss: 0.1817\n",
      "Epoch [11/50], Loss: 0.1611\n",
      "Epoch [12/50], Loss: 0.1474\n",
      "Epoch [13/50], Loss: 0.1422\n",
      "Epoch [14/50], Loss: 0.1373\n",
      "Epoch [15/50], Loss: 0.1294\n",
      "Epoch [16/50], Loss: 0.1229\n",
      "Epoch [17/50], Loss: 0.1171\n",
      "Epoch [18/50], Loss: 0.1123\n",
      "Epoch [19/50], Loss: 0.1087\n",
      "Epoch [20/50], Loss: 0.1062\n",
      "Epoch [21/50], Loss: 0.1031\n",
      "Epoch [22/50], Loss: 0.0999\n",
      "Epoch [23/50], Loss: 0.0980\n",
      "Epoch [24/50], Loss: 0.0962\n",
      "Epoch [25/50], Loss: 0.0949\n",
      "Epoch [26/50], Loss: 0.0929\n",
      "Epoch [27/50], Loss: 0.0911\n",
      "Epoch [28/50], Loss: 0.0898\n",
      "Epoch [29/50], Loss: 0.0884\n",
      "Epoch [30/50], Loss: 0.0870\n",
      "Epoch [31/50], Loss: 0.0864\n",
      "Epoch [32/50], Loss: 0.0867\n",
      "Epoch [33/50], Loss: 0.0855\n",
      "Epoch [34/50], Loss: 0.0836\n",
      "Epoch [35/50], Loss: 0.0816\n",
      "Epoch [36/50], Loss: 0.0801\n",
      "Epoch [37/50], Loss: 0.0796\n",
      "Epoch [38/50], Loss: 0.0789\n",
      "Epoch [39/50], Loss: 0.0788\n",
      "Epoch [40/50], Loss: 0.0787\n",
      "Epoch [41/50], Loss: 0.0786\n",
      "Epoch [42/50], Loss: 0.0780\n",
      "Epoch [43/50], Loss: 0.0768\n",
      "Epoch [44/50], Loss: 0.0760\n",
      "Epoch [45/50], Loss: 0.0750\n",
      "Epoch [46/50], Loss: 0.0743\n",
      "Epoch [47/50], Loss: 0.0739\n",
      "Epoch [48/50], Loss: 0.0737\n",
      "Epoch [49/50], Loss: 0.0731\n",
      "Epoch [50/50], Loss: 0.0729\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 50\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for images, masks in train_loader:\n",
    "        images = images.cuda()\n",
    "        masks = masks.cuda()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)['out']\n",
    "        loss = criterion(outputs, masks.squeeze(1)) # Mask have [batch_size, C, H, W] -> [batch_size, H, W]\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss/len(train_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_to_color(mask, class_colors):\n",
    "    \"\"\"\n",
    "    Converte una maschera con indici di classe in un'immagine RGB.\n",
    "    \n",
    "    Args:\n",
    "        mask (numpy array): Matrice 2D con valori delle classi.\n",
    "        class_colors (dict): Dizionario {indice_classe: (R, G, B)}\n",
    "    \n",
    "    Returns:\n",
    "        numpy array: Maschera colorata in formato RGB.\n",
    "    \"\"\"\n",
    "    h, w = mask.shape\n",
    "    color_mask = np.zeros((h, w, 3), dtype=np.uint8)\n",
    "\n",
    "    for cls, color in class_colors.items():\n",
    "        color_mask[mask == cls] = color\n",
    "\n",
    "    return color_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_class_from_image(image, mask, class_to_remove, replace_with=(0, 0, 0)):\n",
    "    \"\"\"\n",
    "    Rimuove una classe specifica dall'immagine utilizzando la maschera segmentata.\n",
    "    \n",
    "    Args:\n",
    "        image (PIL Image or NumPy array): L'immagine originale (RGB).\n",
    "        mask (torch.Tensor or NumPy array): La maschera delle classi [H, W].\n",
    "        class_to_remove (int): L'etichetta della classe da rimuovere.\n",
    "        replace_with (tuple): Colore RGB con cui sostituire i pixel della classe rimossa.\n",
    "\n",
    "    Returns:\n",
    "        NumPy array: L'immagine modificata.\n",
    "    \"\"\"\n",
    "    # Converte l'immagine in NumPy se è un PIL Image\n",
    "    if isinstance(image, Image.Image):\n",
    "        image = np.array(image) \n",
    "\n",
    "    if isinstance(mask, torch.Tensor):\n",
    "        mask = mask.cpu().numpy() \n",
    "\n",
    "    # Assicura che `image` abbia shape [H, W, 3]\n",
    "    if len(image.shape) == 2:  # Se è in grayscale, converte in RGB\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "    # Se le dimensioni non corrispondono, ridimensiona la maschera\n",
    "    if mask.shape[:2] != image.shape[:2]:\n",
    "        mask = cv2.resize(mask, (image.shape[1], image.shape[0]), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "    # Crea una maschera booleana per i pixel da rimuovere\n",
    "    mask_remove = (mask == class_to_remove)\n",
    "\n",
    "    # Controllo sulle dimensioni\n",
    "    if mask_remove.shape[:2] != image.shape[:2]:\n",
    "        raise ValueError(f\"Dimensioni non corrispondenti! Image: {image.shape}, Mask: {mask_remove.shape}\")\n",
    "\n",
    "    # Rimuove la classe\n",
    "    image[mask_remove] = replace_with\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_background(image, threshold=10):\n",
    "    \"\"\"\n",
    "    Ritaglia i bordi dell'immagine eliminando le aree nere o bianche (o sfondi uniformi).\n",
    "\n",
    "    Args:\n",
    "        image (NumPy array): L'immagine da ritagliare.\n",
    "        threshold (int): Tolleranza per rilevare colori simili al bordo (default=10).\n",
    "\n",
    "    Returns:\n",
    "        NumPy array: L'immagine ritagliata.\n",
    "    \"\"\"\n",
    "    # Converte l'immagine in scala di grigi\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "    # Trova il colore di sfondo (prendiamo il primo pixel in alto a sinistra)\n",
    "    bg_color = gray[0, 0]\n",
    "\n",
    "    # Crea una maschera dove il colore è simile al background\n",
    "    mask = np.abs(gray - bg_color) > threshold\n",
    "\n",
    "    # Trova i contorni della regione utile\n",
    "    coords = np.argwhere(mask)\n",
    "\n",
    "    if coords.size > 0:\n",
    "        y_min, x_min = coords.min(axis=0)\n",
    "        y_max, x_max = coords.max(axis=0)\n",
    "\n",
    "        # Ritaglia l'immagine\n",
    "        cropped_image = image[y_min:y_max+1, x_min:x_max+1]\n",
    "    else:\n",
    "        cropped_image = image  # Se tutto è sfondo, non taglia nulla\n",
    "\n",
    "    return cropped_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Segmenta le immagini "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "image_folder = Path('./results/geo_rectification')\n",
    "\n",
    "model.eval()\n",
    "\n",
    "\n",
    "for image_path in image_folder.glob('*'):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    image_tensor = image_transform(image).unsqueeze(0).cuda()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(image_tensor)['out']\n",
    "        output = torch.argmax(output, dim=1).cpu().numpy()[0]\n",
    "\n",
    "        color_mask = mask_to_color(output, CLASS_COLORS)\n",
    "\n",
    "        plt.imsave('./results/segmentation/' + image_path.name, color_mask)\n",
    "        \n",
    "        \"\"\"\n",
    "        # Visualizza risultati\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(image)\n",
    "        plt.title(\"Immagine Originale\")\n",
    "\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow(color_mask)\n",
    "        plt.title(\"Maschera Predetta (Colorata)\")\n",
    "\n",
    "        plt.show()\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rimuove una classe dalle immagini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "state_dict = torch.load('./results/weights/fcn_resnet50_weights.pth')\n",
    "\n",
    "model.load_state_dict(state_dict)\n",
    "model.eval()\n",
    "\n",
    "image_folder = Path('./data')\n",
    "\n",
    "for image_path in image_folder.glob('*'):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    image_tensor = image_transform(image).unsqueeze(0).cuda()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(image_tensor)['out']\n",
    "        output = torch.argmax(output, dim=1).cpu().numpy()[0]\n",
    "\n",
    "        rem_image = remove_class_from_image(image, output, 1)\n",
    "        cropped_img = crop_background(rem_image)\n",
    "\n",
    "        plt.imsave('./results/no_sky_images/' + image_path.name, cropped_img)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hammon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
