{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Automated Rectification of Image.\n",
    "\n",
    "References\n",
    "----------\n",
    "1.  Chaudhury, Krishnendu, Stephen DiVerdi, and Sergey Ioffe.\n",
    "    \"Auto-rectification of user photos.\" 2014 IEEE International Conference on\n",
    "    Image Processing (ICIP). IEEE, 2014.\n",
    "2.  Bazin, Jean-Charles, and Marc Pollefeys. \"3-line RANSAC for orthogonal\n",
    "    vanishing point detection.\" 2012 IEEE/RSJ International Conference on\n",
    "    Intelligent Robots and Systems. IEEE, 2012.\n",
    "\"\"\"\n",
    "from skimage import feature, color, transform, io\n",
    "import numpy as np\n",
    "import logging\n",
    "from skimage import img_as_ubyte, img_as_float\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_edgelets(image, sigma=3):\n",
    "    \"\"\"Create edgelets as in the paper.\n",
    "\n",
    "    Uses canny edge detection and then finds (small) lines using probabilistic\n",
    "    hough transform as edgelets.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    image: ndarray\n",
    "        Image for which edgelets are to be computed.\n",
    "    sigma: float\n",
    "        Smoothing to be used for canny edge detection.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    locations: ndarray of shape (n_edgelets, 2)\n",
    "        Locations of each of the edgelets.\n",
    "    directions: ndarray of shape (n_edgelets, 2)\n",
    "        Direction of the edge (tangent) at each of the edgelet.\n",
    "    strengths: ndarray of shape (n_edgelets,)\n",
    "        Length of the line segments detected for the edgelet.\n",
    "    \"\"\"\n",
    "    # Check if the image has an alpha channel (4 channels)\n",
    "    if image.shape[-1] == 4:\n",
    "        image = image[..., :3]  # Remove the alpha channel\n",
    "\n",
    "    gray_img = color.rgb2gray(image)  # Convert to grayscale\n",
    "    edges = feature.canny(gray_img, sigma)\n",
    "    lines = transform.probabilistic_hough_line(edges, line_length=3,\n",
    "                                               line_gap=2)\n",
    "\n",
    "    locations = []\n",
    "    directions = []\n",
    "    strengths = []\n",
    "\n",
    "    for p0, p1 in lines:\n",
    "        p0, p1 = np.array(p0), np.array(p1)\n",
    "        locations.append((p0 + p1) / 2)\n",
    "        directions.append(p1 - p0)\n",
    "        strengths.append(np.linalg.norm(p1 - p0))\n",
    "\n",
    "    # Convert to numpy arrays and normalize\n",
    "    locations = np.array(locations)\n",
    "    directions = np.array(directions)\n",
    "    strengths = np.array(strengths)\n",
    "\n",
    "    directions = np.array(directions) / \\\n",
    "        np.linalg.norm(directions, axis=1)[:, np.newaxis]\n",
    "\n",
    "    return (locations, directions, strengths)\n",
    "\n",
    "\n",
    "def edgelet_lines(edgelets):\n",
    "    \"\"\"Compute lines in homogenous system for edglets.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    edgelets: tuple of ndarrays\n",
    "        (locations, directions, strengths) as computed by `compute_edgelets`.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    lines: ndarray of shape (n_edgelets, 3)\n",
    "        Lines at each of edgelet locations in homogenous system.\n",
    "    \"\"\"\n",
    "    locations, directions, _ = edgelets\n",
    "    normals = np.zeros_like(directions)\n",
    "    normals[:, 0] = directions[:, 1]\n",
    "    normals[:, 1] = -directions[:, 0]\n",
    "    p = -np.sum(locations * normals, axis=1)\n",
    "    lines = np.concatenate((normals, p[:, np.newaxis]), axis=1)\n",
    "    return lines\n",
    "\n",
    "\n",
    "def compute_votes(edgelets, model, threshold_inlier=5):\n",
    "    \"\"\"Compute votes for each of the edgelet against a given vanishing point.\n",
    "\n",
    "    Votes for edgelets which lie inside threshold are same as their strengths,\n",
    "    otherwise zero.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    edgelets: tuple of ndarrays\n",
    "        (locations, directions, strengths) as computed by `compute_edgelets`.\n",
    "    model: ndarray of shape (3,)\n",
    "        Vanishing point model in homogenous cordinate system.\n",
    "    threshold_inlier: float\n",
    "        Threshold to be used for computing inliers in degrees. Angle between\n",
    "        edgelet direction and line connecting the  Vanishing point model and\n",
    "        edgelet location is used to threshold.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    votes: ndarray of shape (n_edgelets,)\n",
    "        Votes towards vanishing point model for each of the edgelet.\n",
    "\n",
    "    \"\"\"\n",
    "    vp = model[:2] / model[2]\n",
    "\n",
    "    locations, directions, strengths = edgelets\n",
    "\n",
    "    est_directions = locations - vp\n",
    "    dot_prod = np.sum(est_directions * directions, axis=1)\n",
    "    abs_prod = np.linalg.norm(directions, axis=1) * \\\n",
    "        np.linalg.norm(est_directions, axis=1)\n",
    "    abs_prod[abs_prod == 0] = 1e-5\n",
    "\n",
    "    cosine_theta = dot_prod / abs_prod\n",
    "    cosine_theta = np.clip(cosine_theta, -1, 1)  # Clamp values to [-1, 1]\n",
    "\n",
    "    theta = np.arccos(np.abs(cosine_theta))  # Avoid NaN\n",
    "    theta_thresh = threshold_inlier * np.pi / 180\n",
    "    return (theta < theta_thresh) * strengths\n",
    "\n",
    "\n",
    "def ransac_vanishing_point(edgelets, num_ransac_iter=2000, threshold_inlier=5):\n",
    "    \"\"\"Estimate vanishing point using Ransac.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    edgelets: tuple of ndarrays\n",
    "        (locations, directions, strengths) as computed by `compute_edgelets`.\n",
    "    num_ransac_iter: int\n",
    "        Number of iterations to run ransac.\n",
    "    threshold_inlier: float\n",
    "        threshold to be used for computing inliers in degrees.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    best_model: ndarry of shape (3,)\n",
    "        Best model for vanishing point estimated.\n",
    "\n",
    "    Reference\n",
    "    ---------\n",
    "    Chaudhury, Krishnendu, Stephen DiVerdi, and Sergey Ioffe.\n",
    "    \"Auto-rectification of user photos.\" 2014 IEEE International Conference on\n",
    "    Image Processing (ICIP). IEEE, 2014.\n",
    "    \"\"\"\n",
    "    locations, directions, strengths = edgelets\n",
    "    lines = edgelet_lines(edgelets)\n",
    "\n",
    "    num_pts = strengths.size\n",
    "\n",
    "    arg_sort = np.argsort(-strengths)\n",
    "    first_index_space = arg_sort[:num_pts // 5]\n",
    "    second_index_space = arg_sort[:num_pts // 2]\n",
    "\n",
    "    best_model = None\n",
    "    best_votes = np.zeros(num_pts)\n",
    "\n",
    "    for ransac_iter in range(num_ransac_iter):\n",
    "        ind1 = np.random.choice(first_index_space)\n",
    "        ind2 = np.random.choice(second_index_space)\n",
    "\n",
    "        l1 = lines[ind1]\n",
    "        l2 = lines[ind2]\n",
    "\n",
    "        current_model = np.cross(l1, l2)\n",
    "\n",
    "        if np.sum(current_model**2) < 1 or current_model[2] == 0:\n",
    "            # reject degenerate candidates\n",
    "            continue\n",
    "\n",
    "        current_votes = compute_votes(\n",
    "            edgelets, current_model, threshold_inlier)\n",
    "\n",
    "        if current_votes.sum() > best_votes.sum():\n",
    "            best_model = current_model\n",
    "            best_votes = current_votes\n",
    "            logging.info(\"Current best model has {} votes at iteration {}\".format(\n",
    "                current_votes.sum(), ransac_iter))\n",
    "\n",
    "    return best_model\n",
    "\n",
    "\n",
    "def ransac_3_line(edgelets, focal_length, num_ransac_iter=2000,\n",
    "                  threshold_inlier=5):\n",
    "    \"\"\"Estimate orthogonal vanishing points using 3 line Ransac algorithm.\n",
    "\n",
    "    Assumes camera has been calibrated and its focal length is known.\n",
    "    Parameters\n",
    "    ----------\n",
    "    edgelets: tuple of ndarrays\n",
    "        (locations, directions, strengths) as computed by `compute_edgelets`.\n",
    "    focal_length: float\n",
    "        Focal length of the camera used.\n",
    "    num_ransac_iter: int\n",
    "        Number of iterations to run ransac.\n",
    "    threshold_inlier: float\n",
    "        threshold to be used for computing inliers in degrees.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    vp1: ndarry of shape (3,)\n",
    "        Estimated model for first vanishing point.\n",
    "    vp2: ndarry of shape (3,)\n",
    "        Estimated model for second vanishing point, which is orthogonal to\n",
    "        first vanishing point.\n",
    "\n",
    "    Reference\n",
    "    ---------\n",
    "    Bazin, Jean-Charles, and Marc Pollefeys. \"3-line RANSAC for orthogonal\n",
    "    vanishing point detection.\" 2012 IEEE/RSJ International Conference on\n",
    "    Intelligent Robots and Systems. IEEE, 2012.\n",
    "    \"\"\"\n",
    "    locations, directions, strengths = edgelets\n",
    "    lines = edgelet_lines(edgelets)\n",
    "\n",
    "    num_pts = strengths.size\n",
    "\n",
    "    arg_sort = np.argsort(-strengths)\n",
    "    first_index_space = arg_sort[:num_pts // 5]\n",
    "    second_index_space = arg_sort[:num_pts // 5]\n",
    "    third_index_space = arg_sort[:num_pts // 2]\n",
    "\n",
    "    best_model = (None, None)\n",
    "    best_votes = 0\n",
    "\n",
    "    for ransac_iter in range(num_ransac_iter):\n",
    "        ind1 = np.random.choice(first_index_space)\n",
    "        ind2 = np.random.choice(second_index_space)\n",
    "        ind3 = np.random.choice(third_index_space)\n",
    "\n",
    "        l1 = lines[ind1]\n",
    "        l2 = lines[ind2]\n",
    "        l3 = lines[ind3]\n",
    "\n",
    "        vp1 = np.cross(l1, l2)\n",
    "        # The vanishing line polar to v1\n",
    "        h = np.dot(vp1, [1 / focal_length**2, 1 / focal_length**2, 1])\n",
    "        vp2 = np.cross(h, l3)\n",
    "\n",
    "        if np.sum(vp1**2) < 1 or vp1[2] == 0:\n",
    "            # reject degenerate candidates\n",
    "            continue\n",
    "\n",
    "        if np.sum(vp2**2) < 1 or vp2[2] == 0:\n",
    "            # reject degenerate candidates\n",
    "            continue\n",
    "\n",
    "        vp1_votes = compute_votes(edgelets, vp1, threshold_inlier)\n",
    "        vp2_votes = compute_votes(edgelets, vp2, threshold_inlier)\n",
    "        current_votes = (vp1_votes > 0).sum() + (vp2_votes > 0).sum()\n",
    "\n",
    "        if current_votes > best_votes:\n",
    "            best_model = (vp1, vp2)\n",
    "            best_votes = current_votes\n",
    "            logging.info(\"Current best model has {} votes at iteration {}\".format(\n",
    "                current_votes, ransac_iter))\n",
    "\n",
    "    return best_model\n",
    "\n",
    "\n",
    "def reestimate_model(model, edgelets, threshold_reestimate=5):\n",
    "    \"\"\"Reestimate vanishing point using inliers and least squares.\n",
    "\n",
    "    All the edgelets which are within a threshold are used to reestimate model\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model: ndarry of shape (3,)\n",
    "        Vanishing point model in homogenous coordinates which is to be\n",
    "        reestimated.\n",
    "    edgelets: tuple of ndarrays\n",
    "        (locations, directions, strengths) as computed by `compute_edgelets`.\n",
    "        All edgelets from which inliers will be computed.\n",
    "    threshold_inlier: float\n",
    "        threshold to be used for finding inlier edgelets.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    restimated_model: ndarry of shape (3,)\n",
    "        Reestimated model for vanishing point in homogenous coordinates.\n",
    "    \"\"\"\n",
    "    locations, directions, strengths = edgelets\n",
    "\n",
    "    inliers = compute_votes(edgelets, model, threshold_reestimate) > 0\n",
    "    locations = locations[inliers]\n",
    "    directions = directions[inliers]\n",
    "    strengths = strengths[inliers]\n",
    "\n",
    "    lines = edgelet_lines((locations, directions, strengths))\n",
    "\n",
    "    a = lines[:, :2]\n",
    "    b = -lines[:, 2]\n",
    "    est_model = np.linalg.lstsq(a, b)[0]\n",
    "    return np.concatenate((est_model, [1.]))\n",
    "\n",
    "\n",
    "def remove_inliers(model, edgelets, threshold_inlier=10):\n",
    "    \"\"\"Remove all inlier edglets of a given model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model: ndarry of shape (3,)\n",
    "        Vanishing point model in homogenous coordinates which is to be\n",
    "        reestimated.\n",
    "    edgelets: tuple of ndarrays\n",
    "        (locations, directions, strengths) as computed by `compute_edgelets`.\n",
    "    threshold_inlier: float\n",
    "        threshold to be used for finding inlier edgelets.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    edgelets_new: tuple of ndarrays\n",
    "        All Edgelets except those which are inliers to model.\n",
    "    \"\"\"\n",
    "    inliers = compute_votes(edgelets, model, 10) > 0\n",
    "    locations, directions, strengths = edgelets\n",
    "    locations = locations[~inliers]\n",
    "    directions = directions[~inliers]\n",
    "    strengths = strengths[~inliers]\n",
    "    edgelets = (locations, directions, strengths)\n",
    "    return edgelets\n",
    "\n",
    "\n",
    "def compute_homography_and_warp(image, vp1, vp2, clip=True, clip_factor=3):\n",
    "    \"\"\"Compute homography from vanishing points and warp the image.\n",
    "\n",
    "    It is assumed that vp1 and vp2 correspond to horizontal and vertical\n",
    "    directions, although the order is not assumed.\n",
    "    Firstly, projective transform is computed to make the vanishing points go\n",
    "    to infinty so that we have a fronto parellel view. Then,Computes affine\n",
    "    transfom  to make axes corresponding to vanishing points orthogonal.\n",
    "    Finally, Image is translated so that the image is not missed. Note that\n",
    "    this image can be very large. `clip` is provided to deal with this.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    image: ndarray\n",
    "        Image which has to be wrapped.\n",
    "    vp1: ndarray of shape (3, )\n",
    "        First vanishing point in homogenous coordinate system.\n",
    "    vp2: ndarray of shape (3, )\n",
    "        Second vanishing point in homogenous coordinate system.\n",
    "    clip: bool, optional\n",
    "        If True, image is clipped to clip_factor.\n",
    "    clip_factor: float, optional\n",
    "        Proportion of image in multiples of image size to be retained if gone\n",
    "        out of bounds after homography.\n",
    "    Returns\n",
    "    -------\n",
    "    warped_img: ndarray\n",
    "        Image warped using homography as described above.\n",
    "    \"\"\"\n",
    "    # Find Projective Transform\n",
    "    vanishing_line = np.cross(vp1, vp2)\n",
    "    H = np.eye(3)\n",
    "    H[2] = vanishing_line / vanishing_line[2]\n",
    "    H = H / H[2, 2]\n",
    "\n",
    "    # Find directions corresponding to vanishing points\n",
    "    v_post1 = np.dot(H, vp1)\n",
    "    v_post2 = np.dot(H, vp2)\n",
    "    v_post1 = v_post1 / np.sqrt(v_post1[0]**2 + v_post1[1]**2)\n",
    "    v_post2 = v_post2 / np.sqrt(v_post2[0]**2 + v_post2[1]**2)\n",
    "\n",
    "    directions = np.array([[v_post1[0], -v_post1[0], v_post2[0], -v_post2[0]],\n",
    "                           [v_post1[1], -v_post1[1], v_post2[1], -v_post2[1]]])\n",
    "\n",
    "    thetas = np.arctan2(directions[0], directions[1])\n",
    "    #print(directions)\n",
    "    #print(thetas)\n",
    "    # Find direction closest to horizontal axis\n",
    "    h_ind = np.argmin(np.abs(thetas))\n",
    "\n",
    "    # Find positve angle among the rest for the vertical axis\n",
    "    if h_ind // 2 == 0:\n",
    "        v_ind = 2 + np.argmax([thetas[2], thetas[3]])\n",
    "    else:\n",
    "        v_ind = np.argmax([thetas[2], thetas[3]])\n",
    "\n",
    "    \n",
    "    if h_ind // 2 == 0:\n",
    "        A1 = np.array([[directions[0, h_ind], directions[0, v_ind], 0],\n",
    "                    [directions[1, h_ind], directions[1, v_ind], 0],\n",
    "                    [0, 0, 1]])\n",
    "    else:\n",
    "        A1 = np.array([[directions[0, v_ind], directions[0, h_ind], 0],\n",
    "                    [directions[1, v_ind], directions[1, h_ind], 0],\n",
    "                    [0, 0, 1]])\n",
    "    \n",
    "    print(A1)\n",
    "\n",
    "    # Might be a reflection. If so, remove reflection.\n",
    "    if np.linalg.det(A1) < 0:\n",
    "        A1[:, 0] = -A1[:, 0]\n",
    "\n",
    "    A = np.linalg.inv(A1)\n",
    "\n",
    "    # Translate so that whole of the image is covered\n",
    "    inter_matrix = np.dot(A, H)\n",
    "\n",
    "    cords = np.dot(inter_matrix, [[0, 0, image.shape[1], image.shape[1]],\n",
    "                                  [0, image.shape[0], 0, image.shape[0]],\n",
    "                                  [1, 1, 1, 1]])\n",
    "    cords = cords[:2] / cords[2]\n",
    "    \n",
    "    tx = min(0, cords[0].min())\n",
    "    ty = min(0, cords[1].min())\n",
    "\n",
    "    max_x = cords[0].max() - tx\n",
    "    max_y = cords[1].max() - ty\n",
    "\n",
    "    if clip:\n",
    "        # These might be too large. Clip them.\n",
    "        max_offset = max(image.shape) * clip_factor / 2\n",
    "        tx = max(tx, -max_offset)\n",
    "        ty = max(ty, -max_offset)\n",
    "\n",
    "        max_x = min(max_x, -tx + max_offset)\n",
    "        max_y = min(max_y, -ty + max_offset)\n",
    "\n",
    "    max_x = int(max_x)\n",
    "    max_y = int(max_y)\n",
    "\n",
    "    T = np.array([[1, 0, -tx],\n",
    "                  [0, 1, -ty],\n",
    "                  [0, 0, 1]])\n",
    "\n",
    "    final_homography = np.dot(T, inter_matrix)\n",
    "\n",
    "    warped_img = transform.warp(image, np.linalg.inv(final_homography),\n",
    "                                output_shape=(max_y, max_x))\n",
    "    return warped_img\n",
    "\n",
    "\n",
    "def vis_edgelets(image, edgelets, show=True):\n",
    "    \"\"\"Helper function to visualize edgelets.\"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(image)\n",
    "    locations, directions, strengths = edgelets\n",
    "    for i in range(locations.shape[0]):\n",
    "        xax = [locations[i, 0] - directions[i, 0] * strengths[i] / 2,\n",
    "               locations[i, 0] + directions[i, 0] * strengths[i] / 2]\n",
    "        yax = [locations[i, 1] - directions[i, 1] * strengths[i] / 2,\n",
    "               locations[i, 1] + directions[i, 1] * strengths[i] / 2]\n",
    "\n",
    "        plt.plot(xax, yax, 'r-')\n",
    "\n",
    "    if show:\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def vis_model(image, model, show=True):\n",
    "    \"\"\"Helper function to visualize computed model.\"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    edgelets = compute_edgelets(image)\n",
    "    locations, directions, strengths = edgelets\n",
    "    inliers = compute_votes(edgelets, model, 10) > 0\n",
    "\n",
    "    edgelets = (locations[inliers], directions[inliers], strengths[inliers])\n",
    "    locations, directions, strengths = edgelets\n",
    "    vis_edgelets(image, edgelets, False)\n",
    "    vp = model / model[2]\n",
    "    plt.plot(vp[0], vp[1], 'bo')\n",
    "    for i in range(locations.shape[0]):\n",
    "        xax = [locations[i, 0], vp[0]]\n",
    "        yax = [locations[i, 1], vp[1]]\n",
    "        plt.plot(xax, yax, 'b-.')\n",
    "\n",
    "    if show:\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def rectify_image(image, clip=True, clip_factor=6, algorithm='independent', \n",
    "                  reestimate=False):\n",
    "    \"\"\"Rectified image with vanishing point computed using ransac.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    image: ndarray\n",
    "        Image which has to be rectified.\n",
    "    clip_factor: float, optional\n",
    "        Proportion of image in multiples of image size to be retained if gone\n",
    "        out of bounds after homography.\n",
    "    algorithm: one of {'3-line', 'independent'}\n",
    "        independent ransac algorithm finds the orthogonal vanishing points by\n",
    "        applying ransac twice.\n",
    "        3-line algorithm finds the orthogonal vanishing points together, but\n",
    "        assumes knowledge of focal length.\n",
    "    reestimate: bool\n",
    "        If ransac results are to be reestimated using least squares with\n",
    "        inlers. Turn this off if getting bad results.\n",
    "    Returns\n",
    "    -------\n",
    "    warped_img: ndarray\n",
    "        Rectified image.\n",
    "    \"\"\"\n",
    "    if type(image) is not np.ndarray:\n",
    "        image = io.imread(image)\n",
    "\n",
    "    # Compute all edgelets.\n",
    "    edgelets1 = compute_edgelets(image)\n",
    "\n",
    "    if algorithm == 'independent':\n",
    "        # Find first vanishing point\n",
    "        vp1 = ransac_vanishing_point(edgelets1, 2000, threshold_inlier=5)\n",
    "        if reestimate:\n",
    "            vp1 = reestimate_model(vp1, edgelets1, 5)\n",
    "            \n",
    "        # Remove inlier to remove dominating direction.\n",
    "        edgelets2 = remove_inliers(vp1, edgelets1, 10)\n",
    "\n",
    "        # Find second vanishing point\n",
    "        vp2 = ransac_vanishing_point(edgelets2, 2000, threshold_inlier=5)\n",
    "        if reestimate:\n",
    "            vp2 = reestimate_model(vp2, edgelets2, 5)\n",
    "\n",
    "    elif algorithm == '3-line':\n",
    "        focal_length = None\n",
    "        vp1, vp2 = ransac_3_line(edgelets1, focal_length,\n",
    "                                 num_ransac_iter=3000, threshold_inlier=5)\n",
    "    else:\n",
    "        raise KeyError(\n",
    "            \"Parameter 'algorithm' has to be one of {'3-line', 'independent'}\")\n",
    "\n",
    "    # Compute the homography and warp\n",
    "    warped_img = compute_homography_and_warp(image, vp1, vp2, clip,\n",
    "                                             clip_factor=clip_factor)\n",
    "\n",
    "    return warped_img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enhance_image(image, alpha, beta):\n",
    "\n",
    "    kernel = np.array([[0, -1, 0], [-1, 5, -1], [0, -1, 0]]) \n",
    "\n",
    "    enhanced_image = cv2.addWeighted(image, alpha, image, 0, beta)\n",
    "    #sharpened_image = cv2.filter2D(image, -1, kernel)\n",
    "\n",
    "    return enhanced_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_building(image):\n",
    "    # Se ha 3 canali\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "    # Crea una maschera dei pixel \"non neri\" (threshold a piacere)\n",
    "    mask = gray > 100\n",
    "\n",
    "    # Trova coordinate (r, c) dei pixel non neri\n",
    "    coords = np.argwhere(mask)\n",
    "    y0, x0 = coords.min(axis=0)\n",
    "    y1, x1 = coords.max(axis=0) + 1\n",
    "\n",
    "    # Effettua il crop\n",
    "    cropped = image[y0:y1, x0:x1]\n",
    "    \n",
    "    return cropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_building_improved(image):\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "    print(f\"Cutting {image_path}\")\n",
    "\n",
    "    # Applica una soglia per separare l'edificio (non nero) dallo sfondo\n",
    "    # Se l'edificio è chiaro e lo sfondo è nero, puoi usare:\n",
    "    _, bin_img = cv2.threshold(gray, 20, 255, cv2.THRESH_BINARY)\n",
    "    # (usa 20 come soglia di \"nero\", da aggiustare in base all'immagine)\n",
    "\n",
    "    # Trova le componenti connesse\n",
    "    num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(bin_img, connectivity=8)\n",
    "\n",
    "    # stats[i] = [x_min, y_min, width, height, area]\n",
    "    # label 0 corrisponde allo sfondo (di solito), quindi cerchiamo la componente più grande fra quelle > 0\n",
    "    max_area = 0\n",
    "    max_label = 0\n",
    "    for label in range(1, num_labels):\n",
    "        area = stats[label, cv2.CC_STAT_AREA]\n",
    "        if area > max_area:\n",
    "            max_area = area\n",
    "            max_label = label\n",
    "\n",
    "    # Estraiamo bounding box della componente più grande\n",
    "    x_min = stats[max_label, cv2.CC_STAT_LEFT]\n",
    "    y_min = stats[max_label, cv2.CC_STAT_TOP]\n",
    "    w = stats[max_label, cv2.CC_STAT_WIDTH]\n",
    "    h = stats[max_label, cv2.CC_STAT_HEIGHT]\n",
    "\n",
    "    # Ritaglia l'immagine\n",
    "    cropped = image[y_min:y_min+h, x_min:x_min+w]\n",
    "    return cropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = './data/Building_124.png'\n",
    "\n",
    "image = io.imread(image_path)\n",
    "image = cv2.resize(image, (4048, 4048), interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "# Print a message to indicate the rectification process\n",
    "print(f\"Rectifying {image_path}\")\n",
    "\n",
    "# Sharpen the image\n",
    "kernel = np.array([[0, -1, 0],\n",
    "                [-1, 5, -1],\n",
    "                [0, -1, 0]])  # Sharpening kernel\n",
    "sharpened_image = cv2.filter2D(image, -1, kernel)\n",
    "\n",
    "# Convert back to the format used in your pipeline\n",
    "sharpened_image = img_as_float(sharpened_image)\n",
    "\n",
    "# Proceed with rectification\n",
    "rectified_image = rectify_image(sharpened_image,clip=False, clip_factor=10, algorithm='independent')\n",
    "\n",
    "# Convert the rectified image to uint8 for display\n",
    "rectified_image_uint8 = img_as_ubyte(rectified_image)\n",
    "\n",
    "# Ensure the image has 3 channels\n",
    "if rectified_image_uint8.shape[-1] == 4:\n",
    "    rectified_image_uint8 = rectified_image_uint8[..., :3]\n",
    "\n",
    "result_image = crop_building_improved(rectified_image_uint8)\n",
    "\n",
    "\"\"\"\n",
    "#adjusted_image = enhance_image(result_image, 1.3, 30)\n",
    "ycrcb_image = cv2.cvtColor(result_image, cv2.COLOR_RGB2YCrCb)\n",
    "channels=cv2.split(ycrcb_image)\n",
    "cv2.equalizeHist(channels[0],channels[0])\n",
    "cv2.merge(channels,ycrcb_image)\n",
    "rgb_img = cv2.cvtColor(ycrcb_image,cv2.COLOR_YCrCb2RGB)\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(result_image)\n",
    "plt.axis('off')\n",
    "plt.title(\"Rectified Image\")\n",
    "plt.show()\n",
    "\"\"\"\n",
    "\n",
    "print(rectified_image_uint8.shape)\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(result_image)\n",
    "plt.axis('off')\n",
    "plt.title(\"Rectified Image\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "image_folder = Path('./data')\n",
    "\n",
    "for image_path in image_folder.glob('*'):\n",
    "    try:\n",
    "        image = io.imread(image_path)\n",
    "        image = cv2.resize(image, (4048, 4048), interpolation=cv2.INTER_LINEAR)\n",
    "        \n",
    "        # Print a message to indicate the rectification process\n",
    "        print(f\"Rectifying {image_path}\")\n",
    "\n",
    "        # Sharpen the image\n",
    "        kernel = np.array([[0, -1, 0],\n",
    "                        [-1, 5, -1],\n",
    "                        [0, -1, 0]])  # Sharpening kernel\n",
    "        sharpened_image = cv2.filter2D(image, -1, kernel)\n",
    "\n",
    "        # Convert back to the format used in your pipeline\n",
    "        sharpened_image = img_as_float(sharpened_image)\n",
    "\n",
    "        # Proceed with rectification\n",
    "        rectified_image = rectify_image(sharpened_image, clip_factor=4, algorithm='independent')\n",
    "\n",
    "        # Convert the rectified image to uint8 for display\n",
    "        rectified_image_uint8 = img_as_ubyte(rectified_image)\n",
    "\n",
    "        # Ensure the image has 3 channels\n",
    "        if rectified_image_uint8.shape[-1] == 4:\n",
    "            rectified_image_uint8 = rectified_image_uint8[..., :3]\n",
    "\n",
    "        cropped_rectified_image_uint8 = crop_building_improved(rectified_image_uint8)\n",
    "\n",
    "        plt.imsave('./results/vanishing_points_rectification_RGB/' + image_path.name, cropped_rectified_image_uint8)\n",
    "\n",
    "        \"\"\"\n",
    "        img_rect = Image.fromarray((rectified_image * 255).astype(np.uint8))\n",
    "        img_rect.save(\"res_out.png\")\n",
    "\n",
    "        # Display the rectified image\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        plt.imshow(rectified_image_uint8)\n",
    "        plt.axis('off')\n",
    "        plt.title(\"Rectified Image\")\n",
    "        plt.show()\n",
    "        \"\"\"\n",
    "    except Exception as e:\n",
    "        print(f\"Errore con l'immagine {image_path}: {e}, skippata.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "image_folder = Path('./results/vanishing_points_rectification')\n",
    "\n",
    "for image_path in image_folder.glob('*'):\n",
    "    try:\n",
    "        img = cv2.imread(image_path)\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        print(f\"Cutting {image_path}\")\n",
    "\n",
    "        # Applica una soglia per separare l'edificio (non nero) dallo sfondo\n",
    "        # Se l'edificio è chiaro e lo sfondo è nero, puoi usare:\n",
    "        _, bin_img = cv2.threshold(gray, 20, 255, cv2.THRESH_BINARY)\n",
    "        # (usa 20 come soglia di \"nero\", da aggiustare in base all'immagine)\n",
    "\n",
    "        # Trova le componenti connesse\n",
    "        num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(bin_img, connectivity=8)\n",
    "\n",
    "        # stats[i] = [x_min, y_min, width, height, area]\n",
    "        # label 0 corrisponde allo sfondo (di solito), quindi cerchiamo la componente più grande fra quelle > 0\n",
    "        max_area = 0\n",
    "        max_label = 0\n",
    "        for label in range(1, num_labels):\n",
    "            area = stats[label, cv2.CC_STAT_AREA]\n",
    "            if area > max_area:\n",
    "                max_area = area\n",
    "                max_label = label\n",
    "\n",
    "        # Estraiamo bounding box della componente più grande\n",
    "        x_min = stats[max_label, cv2.CC_STAT_LEFT]\n",
    "        y_min = stats[max_label, cv2.CC_STAT_TOP]\n",
    "        w = stats[max_label, cv2.CC_STAT_WIDTH]\n",
    "        h = stats[max_label, cv2.CC_STAT_HEIGHT]\n",
    "\n",
    "        # Ritaglia l'immagine\n",
    "        cropped = img[y_min:y_min+h, x_min:x_min+w]\n",
    "\n",
    "        plt.imsave('./results/vanishing_points_rectification_cutted/' + image_path.name, cropped)\n",
    "    except Exception as e:\n",
    "        print(f\"Errore con l'immagine {image_path}: {e}, skippata.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hammon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
